---
title: "NEH Grants from the 2000s Data Analysis"
author: "Brian A. Danielak"
format: 
  pdf:
    toc: true
    number-sections: true
  html:
    toc: true
    number-sections: true
editor: visual
---

# Executive Summary

Understanding the impact the NEH has

# Loading and Cleaning the Data

## Loading Necessary Libraries {#sec-loading-necessary-libraries}

```{r}
#| echo: false
#| message: false
library(tidycensus)
library(tidyverse)
library(scales)
library(vroom)
library(xml2)

# Load US Census API Key (http://api.census.gov/data/key_signup.html)
# We use `read_lines` so as not to get a trailing newline character when reading the file
census_api_key <- readr::read_lines(
  "../us-census-api-key.txt",
  n_max = 1  
)
```

## Loading Grant Data from Downloaded XML

```{r}
load_2000s_grants_data <- function(
    path_to_xml = "../data-raw/NEH_Grants2000s_Flat/NEH_Grants2000s_Flat.xml"
  ) {
  
  # First, we check if the rectangularized file already exists.
  if (file.exists("../data-cleaned/grants-data-cleaned.tsv")) {
    
    # If it exists, we load it and return it
    grants_cleaned <- vroom::vroom("../data-cleaned/grants-data-cleaned.tsv")
    return(grants_cleaned)
  
  # Otherwise, we do the expensive thing and parse the XML data
  } else {
    grants_raw <- xml2::read_xml(path_to_xml) |> 
      xml2::as_list()
    
    grants_tibble <- grants_raw$Grants |> 
      lapply(unlist) |> 
      dplyr::bind_rows()

    # Write the rectangularized data out so we can load *it* next time
    grants_tibble |> 
      dplyr::transmute_all(unlist) |> 
      vroom::vroom_write("../data-cleaned/grants-data-cleaned.tsv")
    
    # Load the rectangularized data from file 
    # (`vroom` will help infer column types automatically on load.)
    grants_cleaned <- vroom::vroom("../data-cleaned/grants-data-cleaned.tsv") 
    return(grants_cleaned)
  }
}

# I downloaded and unzipped the 2000s grant data
# https://securegrants.neh.gov/open/data/NEH_Grants2000s_Flat.zip
grants_2000s <- load_2000s_grants_data()
```

## Inspecting the Data

First, let's get an idea of what the data looks like.

```{r}
grants_2000s |> 
  dplyr::glimpse()
```

# Research Question: What's the Per-Capita Total Amount Awarded for Each State?

To answer our research question, we're going to:

1.  Get census data
2.  Join census populations with existing awards data
3.  Visualize the Awards Per-Capita in each state

## Loading Census Data with `tidycensus`

```{r}
#| message: false

# We read my API key from a file, because it's a secret. 
# Reading it from an external file prevents accidentally committing it in git.
my_census_api_key <- readr::read_lines(
  "../us-census-api-key.txt",
  n_max = 1  
)

# Then we tell tidycensus to use my API key
census_api_key(my_census_api_key, install = FALSE)

load_census_data <- function(census_data = "../data-cleaned/2010-census.tsv") {
  # We don't want to keep hitting the API every time our code runs, 
  # so if we already have the data, vroom it and return it.
  if (file.exists(census_data)) {
    return(vroom::vroom(census_data))
  # Otherwise, we hit the API for it, vroom it, and return it
  } else {
    
    # I Had to pick a year for the 2000s NEH grant data, so I settled on 2010.
    get_decennial(
      geography = "state", 
      variables = "P001001",
      year = 2010 
    ) |> vroom::vroom_write(census_data)
    return(vroom::vroom(census_data))
  }
}

state_populations <- load_census_data()

states <- state_populations |> 
  rename(
    census_state = NAME,
    population =  value
  )
```

## Augmenting NEH Data with U.S. Census Data

Unfortunately, R's built-in `state` dataset only has data for the 50 states and not other US territories/regions.
So, let's find out which census regions/territories don't exist in R's built-in set.
Then we can add them.

```{r}
# First we grab and store all the unique values in the Census states.
census_state_names <- states |> 
  select(census_state) |> 
  pull() |>
  unique()

# Then, we use them to index into state abbreviations to find out which 
# aren't in R's built-in dataset
states_in_census_but_not_in_built_in_dataset <- is.na(state.abb[match(census_state_names, state.name)])

census_state_names[states_in_census_but_not_in_built_in_dataset]
```

OK, now that we know those states, we can augment the existing state dataset with those abbreviations.

```{r}
census_states <- state.name

# This is a trick that lets us more easily index state names by their abbreviations
names(census_states) <- state.abb

# Now we can add DC and Puerto Rico
census_states <- c(
  census_states, 
  DC = "District of Columbia",
  PR = "Puerto Rico"
)
```

Next, we use our newly-augmented `census_states` to see how many grants would get dropped because we don't have US Census populations for them.

```{r}
grants_with_no_population_info <- grants_2000s |> 
  filter(!(InstState %in% names(census_states)))

number_of_grants_with_no_population_data <- dim(grants_with_no_population_info)[1]

percentage_of_grants_without_population_data <- round(dim(grants_with_no_population_info)[1] / dim(grants_2000s)[1] * 100, 2)
```

The original dataset had `r dim(grants_2000s)[1]` rows.
There are `r number_of_grants_with_no_population_data` grants in states with no populations.
So, if we drop them, we're losing `r percentage_of_grants_without_population_data` percent of our data.

## Augmenting 2000s Grants Data with 2010 Census Data

```{r}
grants_with_population_data <- grants_2000s |> 
  filter(InstState %in% names(census_states))

indexed_census_states <- census_states[grants_with_population_data$InstState]
  
  # We index into the census states vector to grab full state names from abbreviations,
  # then we augment the dataframe with that new column
  grants_with_population_data <- grants_with_population_data |> 
    mutate(
      census_state = indexed_census_states,
  ) |> 
  left_join(states, by = join_by(census_state))
```

## Computing the Total Amount Awarded

Here, we augment the data again with my best sense of the **total amount** of a grant awarded.
In my thinking, that would be the `AwardOutright` plus matching funds (`AwardMatching`).

```{r}
grants_with_population_data <- grants_with_population_data |>
  mutate(grant_total = AwardOutright + AwardMatching)

grants_with_population_data |> 
  select(census_state)

# grants_with_population_data |> 
#   select(InstState, census_state, population, grant_total)
```

## Computing the Dollars Awarded Per Capita in Each State

```{r}
awards_per_capita_by_state <- grants_with_population_data |> 
  select(census_state, grant_total, population) |> 
  group_by(census_state, population) |> 
  summarize(
    total_awarded_to_state = (sum(grant_total))) |> 
  mutate(per_capita_award = total_awarded_to_state / population)

awards_per_capita_by_state

```

## Visualizing Awards Per-Capita in Each State

```{r}
#| fig-height: 10
#| fig-width: 8

national_per_capita_award <- (
  sum(awards_per_capita_by_state$total_awarded_to_state) / 
  sum(awards_per_capita_by_state$population)
)

national_award_total <- awards_per_capita_by_state$total_awarded_to_state |> sum()

awards_per_capita_by_state |>
  ggplot() +
  geom_point(
    aes(x = forcats::fct_reorder(census_state, per_capita_award),
        y = per_capita_award,
        size = population,
        color = ((total_awarded_to_state / national_award_total) * 100)
    )
  ) +
  coord_flip() +
  geom_hline(
    yintercept = national_per_capita_award,
    color = "darkorange",
    alpha = 0.75,
    
  ) +
  scale_color_viridis_c(name = "% of NEH Award Budget") + 
  scale_size_continuous(name = "State Population") + 
  scale_y_continuous(
    name = "Per Capita Award ($USD)", 
    labels = scales::label_dollar()
  ) +
  scale_x_discrete(name = "US State/Region") +
  labs(
    title = "Exploring NEH Award Dollars Spent Per-Capita",
    subtitle = "Broken down by US States and Territories",
    caption = "The mean national per-capita NEH award amount is indicated by the orange vertical line."
  ) +
  theme_minimal()
```

# Analysis and Interpretation

# Key Take-Away

# Limitations

## We Excluded Certain US Regions and Territories

Not all parts of the United States are represented in this analysis.
While we only excluded `percentage_of_grants_without_population_data` percent of the observations in the data, that still excludes entire US regions and territories.
One thing we can do is determine how much **money** in grants is being awarded to those territories and thus being excluded from our analysis.

```{r}
total_awarded_to_all_us_territories <- grants_2000s |> 
  select(AwardOutright) |> 
  sum()

total_awarded_to_non_us_territories <- grants_with_no_population_info |> 
  select(AwardOutright) |> 
  sum()

percent_awards_excluded <- 
  ((total_awarded_to_non_us_territories / total_awarded_to_all_us_territories) * 100) |> 
  round(2)
```

In the 2000s the NEH awarded \$`r total_awarded_to_all_us_territories` dollars, of which \$`r total_awarded_to_non_us_territories` dollars were excluded from this analysis.
That exclusion represents about `r percent_awards_excluded` percent of NEH's total awards.
While that percentage is small, those awards may have had a real impact on the residents of those regions.
That potential impact isn't captured in this analysis.
Ultimately, because of our exclusion, we can't say with certainty what the NEH award dollars per capita are in those regions.

## There are Some Limits to the Reproducibility of This Document

-   To reproduce these findings on another machine, the user would need to get their own [Census API Token](http://api.census.gov/data/key_signup.html).
-   I could have done more to make the package environment entirely reproducible. As it is, users will have to install the packages I listed in @sec-loading-necessary-libraries.

# TODO

-   Set echo = False for most of the code chunks.
-   Flesh out Analysis and Key Take-Away sections.
-   Check code for lines that are too long

# Sources

-   `fct_reorder` in R package `forcats` documentation: [*https://forcats.tidyverse.org/reference/fct_reorder.html*](https://forcats.tidyverse.org/reference/fct_reorder.html)
-   `as_list` in R package `xml2`: <https://xml2.r-lib.org/reference/as_list.html>
-   StackOverflow: Converting a State Name to State Abbreviation: [*https://stackoverflow.com/questions/5411979/state-name-to-abbreviation*](https://stackoverflow.com/questions/5411979/state-name-to-abbreviation){.uri}
-   Analyzing US Census Data: Methods, Maps, and Models in R: [*https://walker-data.com/census-r/an-introduction-to-tidycensus.html?q=key#getting-started-with-tidycensus*](https://walker-data.com/census-r/an-introduction-to-tidycensus.html?q=key#getting-started-with-tidycensus){.uri}
-   US Census API Key Request Form: [*http://api.census.gov/data/key_signup.html*](http://api.census.gov/data/key_signup.html){.uri}
