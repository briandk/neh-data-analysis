---
title: "NEH Grants from the 2000s Data Analysis"
author: "Brian A. Danielak"
format: 
  pdf:
    toc: true
    number-sections: true
  html:
    toc: true
    number-sections: true
  revealjs: 
    number-sections: true
    embed-resources: true
editor: visual
---

# Executive Summary

Understanding the impact the NEH has

# Loading and Cleaning the Data

## Loading Necessary Libraries {#sec-loading-necessary-libraries}

```{r}
#| echo: true
suppressMessages(
  {
    library(tidycensus)
    library(forcats)
    library(here)
    library(tidyverse)
    library(scales)
    library(vroom)
    library(xml2)
    # library(pillar)
  }
)


# Load US Census API Key (http://api.census.gov/data/key_signup.html)
# Committing the API key would be a security hazard, so we
# read it from an external, non-version-controlled file.
# We use `read_lines()` so as not to get a trailing newline 
# character when reading the file.
my_census_api_key <- readr::read_lines(
  here("us-census-api-key.txt"),
  n_max = 1  
)
```

## Loading Grant Data from Downloaded XML

```{r}
#| echo: true
#| message: false
load_grants_data <- function(
    year = 2000,
    path_to_xml = sprintf(
      here("data-raw/NEH_Grants%s_Flat/NEH_Grants%ss_Flat.xml"), 
      year, 
      year
    )
  ) {
  
  # First, we check if the rectangularized file already exists.
  if (file.exists(here("data-cleaned/grants-data-cleaned.tsv"))) {
    
    # If it exists, we vroom it and return it
    vroom(here("data-cleaned/grants-data-cleaned.tsv"))
  
  # Otherwise, we do the expensive thing and parse the XML data
  } else {
    grants_raw <- read_xml(path_to_xml) |> 
      as_list()
    
    grants_tibble <- grants_raw$Grants |> 
      lapply(unlist) |> 
      bind_rows()

    # Write the rectangularized data out so we can load *it* next time.
    # `vroom::write` returns whatever it loads, so that's what the function
    # will return if it takes the `else` branch
    grants_tibble |> 
      transmute_all(unlist) |> 
      vroom_write(here("data-cleaned/grants-data-cleaned.tsv"))
  }
}

# I downloaded and unzipped the 2000s grant data
# https://securegrants.neh.gov/open/data/NEH_Grants2000s_Flat.zip
grants_2000s <- load_grants_data()
```

## Inspecting the Data

First, let's get an idea of what the data looks like.

```{r}
#| echo: true
grants_2000s |> 
  dplyr::glimpse()
```

# Research Question: What's the Per-Capita Total Amount Awarded for Each State?

## Analysis Overview

To answer our research question, we're going to:

1.  Get census data
2.  Join census populations with existing awards data
3.  Visualize the Awards Per-Capita in each state

## Loading Census Data with `tidycensus`

```{r}
#| message: false
#| echo: true


# Then we tell tidycensus to use my API key
census_api_key(my_census_api_key, install = FALSE)

load_census_data <- 
  function(census_data = here("data-cleaned/2010-census.tsv")) {
    # We don't want to keep hitting the API 
    # every time our code runs, 
    # so if we already have the data, vroom it and return it.
    if (file.exists(census_data)) {
      vroom(census_data)
      # Otherwise, 
      # we hit the API for it, vroom it, and return it
    } else {
      # I Had to pick a year for the 2000s NEH grant data, 
      # so I settled on 2010.
      get_decennial(
        geography = "state", 
        variables = "P001001",
        year = 2010 
      ) |> 
        vroom_write(census_data)
    }
  }

state_populations <- load_census_data()

states <- state_populations |> 
  rename(
    census_state = NAME,
    population =  value
  )
```

## Determining Which NEH Award States/Territories Aren't In the Census Data

Unfortunately, R's built-in `state` dataset only has data for the 50 states and not other US territories/regions.
So, let's find out which census regions/territories don't exist in R's built-in set.
Then we can add them.

```{r}
#| echo: true

# First we grab and store all the unique values in the Census states.
census_state_names <- states |> 
  select(census_state) |> 
  pull() |>
  unique()

# Then, we use them to index into state abbreviations to find out which 
# aren't in R's built-in dataset
# Could use `setdiff()`
states_in_census_but_not_in_built_in_dataset <- 
  setdiff(census_state_names, state.name)

states_in_census_but_not_in_built_in_dataset
```

## Augmenting the `state` Dataset with Census State/Territory Abbreviations

OK, now that we know those states, we can augment the existing state dataset with those abbreviations.

```{r}
#| echo: true

census_states <- state.name

# This is a trick that lets us more easily index state names by their abbreviations
names(census_states) <- state.abb

# Now we can add DC and Puerto Rico
census_states <- c(
  census_states, 
  DC = "District of Columbia",
  PR = "Puerto Rico"
)
```

## How Much NEH Awards Data Are We Losing Because of Missing US States/Regions in the Census?

We use our newly-augmented `census_states` to see how many grants would get dropped because we don't have US Census populations for them.

```{r}
#| echo: true

`%notin%` <- Negate(`%in%`)
grants_with_no_population_info <- grants_2000s |> 
  # We're looking for NEH award regions *not* in our census data
  filter((InstState %notin% names(census_states)))

number_of_grants_with_no_population_data <- nrow(grants_with_no_population_info)

percentage_of_grants_without_population_data <- 
  round(nrow(grants_with_no_population_info) / nrow(grants_2000s) * 100, 2)
```

The original dataset had `r nrow(grants_2000s)` rows.
There are `r number_of_grants_with_no_population_data` grants in states with no populations.
So, if we drop them, we're losing `r percentage_of_grants_without_population_data` percent of our data.

## Augmenting 2000s Grants Data with 2010 Census Data

Now that we know the consequences of dropping data, we can try joining the census data on our NEH awards data.

```{r}
#| echo: true

grants_with_population_data <- 
  grants_2000s |> 
  filter(InstState %in% names(census_states))

# You can't subset within a base-R pipe, so we create this variable
indexed_census_states <- census_states[grants_with_population_data$InstState]
  
  # We index into the census states vector to grab full state names from abbreviations,
  # then we augment the dataframe with that new column
  grants_with_population_data <- 
    grants_with_population_data |> 
    mutate(
      census_state = indexed_census_states,
    ) |> 
    left_join(states)
```

## Computing the Total Amount Awarded

Here, we augment the data again with my best sense of the **total amount** of a grant awarded.
In my thinking, that would be the `AwardOutright` plus matching funds (`AwardMatching`).[^1]

[^1]: This computation is based on my understanding of the column names as described in the dataset's data dictionary.

```{r}
#| echo: true

grants_with_population_data <- 
  grants_with_population_data |>
  mutate(grant_total = AwardOutright + AwardMatching)
```

## Computing the Dollars Awarded Per-Capita in Each State

Next we:

1.  Group by state
2.  Sum the total awards for each state
3.  Create a new column dividng those totals by the state population
4.  Check the output to make sure the transformation worked

Then we check the first few rows to make sure the transformation worked.

```{r}
#| echo: true

awards_per_capita_by_state <- 
  grants_with_population_data |> 
  select(census_state, grant_total, population) |> 
  group_by(census_state, population) |> 
  summarize(
    total_awarded_to_state = (sum(grant_total))
  ) |> 
  mutate(per_capita_award = total_awarded_to_state / population) |>
  arrange(desc(per_capita_award))

awards_per_capita_by_state |> 
  glimpse()
```

## Visualizing Awards Per-Capita in Each State

Now we want to be able to compare states to each other based on the number of dollars the NEH awarded in grants per-capita in each state.
Dividing the total awarded amount to the state by the population gives us a normalized number of dollars spent per person.
In other words, we ask, "how much was awarded in each state on a per-person basis. Computing things this way helps us identify states where NEH awards are high vs. low without being distorted by state population.

```{r}
#| fig-height: 10
#| fig-width: 8
#| echo: true

national_per_capita_award <- (
  sum(awards_per_capita_by_state$total_awarded_to_state) / 
  sum(awards_per_capita_by_state$population)
)

pull(awards_per_capita_by_state, population)
pull(awards_per_capita_by_state, total_awarded_to_state)

national_award_total <- sum(awards_per_capita_by_state$total_awarded_to_state)

awards_per_capita_by_state |>
  ggplot() +
  geom_point(
    aes(x = forcats::fct_reorder(census_state, per_capita_award),
        y = per_capita_award,
        size = population,
        color = ((total_awarded_to_state / national_award_total) * 100)
    )
  ) +
  coord_flip() +
  scale_size_continuous(
    name = "State Population",
    labels = scales::label_comma()
  ) +
  geom_hline(
    yintercept = national_per_capita_award,
    color = "darkorange",
    alpha = 0.75,

  ) +
  scale_color_viridis_c(name = "% of NEH Award Budget") + 
  scale_y_continuous(
    name = "Per Capita Award ($USD)", 
    labels = scales::label_dollar()
  ) +
  # scale_x_discrete(name = "US State/Region") +
  labs(
    x = "US State or Region",
    title = "Exploring Total Award Dollars Per-Capita",
    subtitle = "Broken down by US States and Territories",
    caption = "The mean national per-capita NEH award amount is indicated by the orange vertical line."
  ) +
  theme_bw()
```

# Analysis and Interpretation

DC has the highest NEH dollars awarded per-person number than any other state in the US.
Could this be because DC has such a small population?
Could it be that the number of awards in DC is simply greater than the number of awards in other states?

# Key Take-Away

# Limitations

## The dollar amounts are unadjusted for inflation

## We Excluded Certain US Regions and Territories

Not all parts of the United States are represented in this analysis.
While we only excluded `percentage_of_grants_without_population_data` percent of the observations in the data, that still excludes entire US regions and territories.
One thing we can do is determine how much **money** in grants is being awarded to those territories and thus being excluded from our analysis.

```{r}
#| echo: true

total_awarded_to_all_us_territories <- grants_2000s |> 
  select(AwardOutright) |> 
  sum()

total_awarded_to_non_us_territories <- grants_with_no_population_info |> 
  select(AwardOutright) |> 
  sum()

percent_awards_excluded <- 
  ((total_awarded_to_non_us_territories / total_awarded_to_all_us_territories) * 100) |> 
  round(2)
```

In the 2000s the NEH awarded \$`r total_awarded_to_all_us_territories` dollars, of which \$`r total_awarded_to_non_us_territories` dollars were excluded from this analysis.
That exclusion represents about `r percent_awards_excluded` percent of NEH's total awards.
While that percentage is small, those awards may have had a real impact on the residents of those regions.
That potential impact isn't captured in this analysis.
Ultimately, because of our exclusion, we can't say with certainty what the NEH award dollars per capita are in those regions.

## We used the total award plus the total matching, as opposed to just the amount the NEH spent on the award.

## There are Some Limits to the Reproducibility of This Document

-   To reproduce these findings on another machine, the user would need to get their own [Census API Token](http://api.census.gov/data/key_signup.html).
-   I could have done more to make the package environment entirely reproducible. As it is, users will have to install the packages I listed in @sec-loading-necessary-libraries.

# TODO

-   Set echo = False for most of the code chunks.
-   Flesh out Analysis and Key Take-Away sections.
-   Check code for lines that are too long
-   Fix section 3.7 by removing the `pull()` calls.

# Sources

-   `fct_reorder` in R package `forcats` documentation: [*https://forcats.tidyverse.org/reference/fct_reorder.html*](https://forcats.tidyverse.org/reference/fct_reorder.html)
-   `as_list` in R package `xml2`: <https://xml2.r-lib.org/reference/as_list.html>
-   StackOverflow: Converting a State Name to State Abbreviation: [*https://stackoverflow.com/questions/5411979/state-name-to-abbreviation*](https://stackoverflow.com/questions/5411979/state-name-to-abbreviation){.uri}
-   Analyzing US Census Data: Methods, Maps, and Models in R: [*https://walker-data.com/census-r/an-introduction-to-tidycensus.html?q=key#getting-started-with-tidycensus*](https://walker-data.com/census-r/an-introduction-to-tidycensus.html?q=key#getting-started-with-tidycensus){.uri}
-   US Census API Key Request Form: [*http://api.census.gov/data/key_signup.html*](http://api.census.gov/data/key_signup.html){.uri}
